---
title: "Homework Sheet 4 -- Model comparison and hypothesis testing"
date: 'Due: Wednesday, January 13 by 11:59 CET'
author: "39"
output: 
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)

```

```{r libraries, include=FALSE, message=FALSE, warning=FALSE}

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for this course
library(aida)

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
} 

# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      cache = TRUE, fig.align = 'center')


```

# Instructions

* Create an Rmd-file with your group number (equivalent to StudIP group) in the 'author' heading and answer the following questions.
* When all answers are ready, 'Knit' the document to produce a HTML file.
* Create a ZIP archive called "IDA_HW04-Group-XYZ.zip" (where 'XYZ' is *your* group number) containing:
   * an R Markdown file "IDA_HW04-Group-XYZ.Rmd"
   * a knitted HTML document "IDA_HW04-Group-XYZ.html"
   * any other files necessary to compile your Rmarkdown to HTML (data, pictures etc.)
* Upload the ZIP archive on Stud.IP in your group folder before the deadline (see above). You may upload as many times as you like before the deadline, only your final submission will count.

# <span style = "color:firebrick">Exercise 1:</span> Inspect, preprocess & plot the data [15 points]

## Ex 1.a Load and inspect the data [1 point]

Thanks to everyone who had time and energy to complete the Simon task, we have data to analyze fresh data generated by yourselves.
Load the data you and/or your peers generated into a variable called `data_ST` and display the first seven rows of the data set.

**Solution:** 

```{r}
data_ST <- read_csv("data-Simon-Task-minimal.csv")
head(data_ST, n=7)
```

## Ex 1.b Determine the number of participants [1 point]

Use R to extract the number of participants in the data set.

**Solution:**

```{r}
d <- data_ST %>% 
  group_by(submission_id) %>% 
  summarise(number =n ())
nrow(d)
#39 participants, 120 trials for each. 39x120 = 4,680
```

## Ex 1.c Identify variables of interest [2 points]

Based on the hypotheses formulated in [Chapter D.2 of the web-book](https://michael-franke.github.io/intro-data-analysis/simon-task.html), which of the columns in the data tibble represent the main indendent and dependent variables of interest?

**Solution:**

\# TODO: unsure if key_p_is_for counts as an independent variable or not
independent: key_p_is_for, condition
dependent: correctness, RT

## Ex 1.d By-participant mean correctness and mean RT [2 points]

Create a tibble in which each row shows the proportion of "correct" answers and the mean reaction time for a different participant. So, this tibble will have as many rows as there are participants in the data set. (**Hint:** Use a sequence of `group_by` and `summarize`.)

Plot the by-participant summary statistics in this tibble in a scatter plot, showing the proportion of "correct" answers on the $x$-axis and the mean reaction times on the $y$-axis. (Just for clarity: Every point in this plot represents one participant.)

(Rationale for doing this: Even though we will not draw direct conclusions from this plot here, visualizing your data like shown here is usually a good way of getting familiar with what you are dealing with.))

**Solution:**
```{r}
d_summary <- data_ST %>% 
  group_by(submission_id) %>% 
  summarize(mean_RT = mean(RT),
            proportion_C = mean(ifelse(correctness == "correct", 1, 0)))

d_summary

d_summary %>% 
  ggplot(aes(x = proportion_C, y = mean_RT)) + 
  geom_point()
```

## Ex 1.e Remove participants [3 points]

Remove all data from all participants who gave less than 50% correct answers or whose mean reaction time is smaller than 350ms or larger than 750ms.
Use R to output information about how many participants have been excluded in this way.

**Solution:**
```{r}
d_summary <- d_summary %>% 
  mutate(exclude = case_when(mean_RT < 350 ~ TRUE,
                             mean_RT > 750 ~ TRUE,
                             proportion_C < 0.5 ~ TRUE,
                            TRUE ~ FALSE))

#Data visualization - to see the excluded participants easily 
d_summary %>% 
  ggplot(aes(x = proportion_C, y = mean_RT)) +
  geom_point() + 
  geom_point(data = filter(d_summary, exclude == TRUE),
             color = "skyblue", shape = "square", size = 2)

#1 participant has been excluded
```

## Ex 1.f Remove individual trials [3 points]

Exclude all trials with reaction times longer than 1 second or shorter than 100 ms.
Use R to output information about how many trials have been excluded in this way.

**Solution:**

```{r}
original_num <- nrow(data_ST)
data_ST <- filter(data_ST, RT > 100 & RT < 1000)
data_ST
#nrow before the remove - nrow after the remove = the number of trials have been excluded.
#4680 - 4565 = 115
#115 trials have been excluded
original_num - nrow(data_ST)
```

```{r}
data_ST %>% 
  ggplot(aes(x = RT)) +
  geom_histogram() +
  geom_jitter(aes(x = RT, y = 1), alpha = 0.2, height = 200)

message(
  "exclude", 
  nrow(filter(data_ST, RT <100)) + nrow(filter(data_ST, RT >100)), "trials based on too fast or too slow RTs")
data_ST <- filter(data_ST, RT >100 & RT <1000)
data_ST %>% 
  ggplot(aes(x = RT)) + 
  geom_histogram() + 
  geom_jitter(aes(x = RT, y = 1), alpha = 0.2, height = 200)
```

## Ex 1.g Show summary statistics [3 points]

Produce a tibble which shows in each row, one row for each condition, the overall (group-level) proportion of correct answers and the mean of all reaction times per condition.

What would you conclude based on these summary statistics about either or both of the relevant research hypotheses, as spelled out in [Chapter D.2 of the web-book](https://michael-franke.github.io/intro-data-analysis/simon-task.html)?

**Solution:**
```{r}
data_Summary <- data_ST %>% 
  group_by(condition) %>% 
  summarise(proportion_C = mean(correctness == "correct"), mean_RT = mean(RT))
data_Summary
# proportion of correct answers is higher in the congruent trials
# mean of reaction times is higher in the incongruent trials
# As we have to see if these differences are meaningful, we withhold judgement 
# for now
```

# <span style = "color:firebrick">Exercise 2:</span> Testing the accuracy hypothesis [20 points]

## Ex 2.a A model for the accuracy difference [2 points]

We want to test the directed hypothesis that the accuracy (i.e., the proportion of correct responses) is higher in the congruent condition than in the incongruent condition.
To do so, we use a model which is very similar to the model we used for the BioNTec/Pfizer data from the last homework.
The only difference is that we are not interested in the "efficiency" of a treatment, but in the plain difference $\delta = \theta_1 - \theta_2$ between the latent accuracy scores $\theta_1$ (for the congruent condition) and $\theta_2$ (for the incongruent condition).

Here is the model in math.

$$
\begin{align*}
  \theta_1 & \sim \text{Beta}(1,1) \\
  k_1 & \sim \text{Binomial}(\theta_1, N_1) \\
  \theta_1 & \sim \text{Beta}(1,1) \\
  k_2 & \sim \text{Binomial}(\theta_2, N_2) \\
  \delta & = \theta_1 - \theta_2
\end{align*}
$$

Based on the conventions used in the webbook, draw a graphical representation of this model and include it using Rmarkdown into the HTML output of your submission. You can use a photo of a hand-drawn model, or use any kind of software. Just donâ€™t waste too much time on the aesthetics! Make sure to include the picture in your ZIP-submission and integrate.

![Graphical Representation.](./HW04Graph.jpg)

## Ex 2.b Specify the accuracy hypothesis formally [1 points]

State the *accuracy hypothesis* (that the accuracy (i.e., the proportion of correct responses) is higher in the congruent condition than in the incongruent condition) formally as a mathematical statement about a single variable of the model from above.

**Solution:**

$$
\delta > 0
$$

## Ex 2.c Testing via estimation and Kruschke's ternary decision logic [10 points]

Based on the model from above, we will test the accuracy hypothesis, using an estimation-based approach, based on 95% credible intervals and Kruschke's ternary decision logic for our final conclusions.

To do this, follow these steps:

1. get the relevant counts:
  - number of "correct" answers $k_1$ out of $N_1$ congruent trials
  - number of "correct" answers $k_2$ out of $N_2$ incongruent trials

2. get 10,000 samples from the posterior of the latent parameters $\theta_1$ and $\theta_2$ using conjugacy

3. compute (derived) samples from the posterior of $\delta$

4. compute the 95% credible interval for the vector of samples of posterior values of $\delta$

5. draw conclusions from this computation about the research hypothesis, following Kruschke's ternary decision logic

**Solution:**
```{r}
#1
simon_data_4_Stan <- list(
  N1 = nrow(data_ST %>% filter(condition == "congruent")),
  k1 = nrow(data_ST %>% filter(condition == "congruent") %>% filter(correctness == "correct")),
  N2 = nrow(data_ST %>% filter(condition == "incongruent")),
  k2 = nrow(data_ST %>%  filter(condition == "incongruent") %>% filter(correctness == "correct"))
)
simon_data_4_Stan

#2
samples_1 <- rbeta(10000, 2207, 97)
samples_2 <- rbeta(10000, 2069, 196)

#3
samples_delta <- samples_1 - samples_2

#4
aida::summarize_sample_vector(samples_delta, name = 'delta')

#5 - The directional hypothesis delta > 0 contains the 95% credible interval in its entirety. Therefore, we would following the ternary decision logic, accept this hypothesis based on the data.
```

## Ex 2.d Testing via comparison using an embedding model [7 points]

We want to use the encompassing models approach to calculate the Bayes factor of the accuracy hypothesis compared against its logical complement.
Notice that the model we specified in the first part of this exercise *is* an encompassing model which contains the accuracy hypothesis and its logical complement as special cases.
As described in the web-book, we can use the following formula to compute the relevant Bayes factor.

$$ 
\begin{aligned}
\text {BF}_{01} & = \frac{\text{posterior-odds of } H_0}{\text{prior-odds of } H_0}  \\
& = \frac{P_M(\theta \in I_0 \mid D)}{P_M(\theta \not \in I_0 \mid D)} \frac{P_M(\theta \not \in I_0)}{P_M(\theta \in I_0)}
\end{aligned}
$$

We are going to compute this quantity, approximately, using samples, following these steps.

First, reason in intuitive (informal) language why we can safely neglect the prior odds term

$$ 
\begin{aligned}
\frac{P_M(\theta \not \in I_0)}{P_M(\theta \in I_0)}
\end{aligned}
$$

You could also give a mathematical proof, or any other (informal or formal) argument to support your case. (E.g., maybe a sampling-based argument.)

**Solution:**

Second, use the samples in `delta_samples` from the previous exercise to approximate:

$$ 
\begin{aligned}
\frac{P_M(\theta \in I_0 \mid D)}{P_M(\theta \not \in I_0 \mid D)}
\end{aligned}
$$

**Solution:**

Third, interpret this result.

**Solution:**


