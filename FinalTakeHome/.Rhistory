library(aida)
# package for Bayesian regression
library(brms)
# parallel execution of Stan code
options(mc.cores = parallel::detectCores())
# use the aida-theme for plotting
theme_set(theme_aida())
# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
# setting theme colors globally
scale_colour_discrete <- function(...) {
scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
scale_fill_manual(..., values = project_colors)
}
# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
cache = TRUE, fig.align = 'center')
d <- read_csv('BM_data.csv')
num_participants <- d %>%
group_by(Subj) %>%
nrow
num_condition <- d %>%
group_by(Cond) %>%
count()
num_NV <- num_condition[1,2]
num_VB <- num_condition[2,2]
d_tidy <- d %>%
select(., -seq(26,35)) %>%
pivot_longer(., cols = seq(6,25), names_to = "trial_label", values_to = "response")
d_tidy
# TODO check for which decision the CA strategy would make for 1-5 and 6-10 respectively (brain tired)
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(trial_type = case_when(
str_detect(trial_label, "A") ~ "category_A",
str_detect(trial_label, "B") ~ "category_B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "exception_2",
str_detect(trial_label, "T[1-5]") ~ "exception_1",
str_detect(trial_label, "T[6-9]") ~ "exception_2",
TRUE ~ "error"
))
d_tidy
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(expected_response = case_when(
str_detect(trial_label, "A") ~ "A",
str_detect(trial_label, "B") ~ "B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "A",
str_detect(trial_label, "T[1-5]") ~ "B",
str_detect(trial_label, "T[6-9]") ~ "A",
TRUE ~ "error"
))
d_ca_check <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]")) %>%
group_by(Subj) %>%
mutate(CA_check = mean(as.character(response) == as.character(expected_response)))
d_accuracy_check <- d_tidy %>%
filter(str_detect(trial_label, "[AB]")) %>%
group_by(Subj) %>%
mutate(Accuracy_check = mean(as.character(response) == as.character(expected_response)))
d_ca_check
d_accuracy_check
all(d_ca_check["CA_check"] == d_ca_check["CA"])
all(d_accuracy_check["Accuracy_check"] == d_accuracy_check["Accuracy"])
# your output should conform to this form-template
tribble(
~"measure", ~"Cond", ~"lower", ~"mean", ~"upper",
"Trials", "NV", "...", "...", "...",
"Trials", "VB", "...", "...", "...",
"Accuracy", "NV", "...", "...", "...",
"Accuracy", "VB", "...", "...", "...",
"CA", "NV", "...", "...", "...",
"CA", "VB", "...", "...", "..."
)
# Solution:
d_VB <- d_tidy %>%
filter(Cond == "VB") %>%
group_by(Subj) %>%
summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))
d_NV <- d_tidy %>%
filter(Cond == "NV") %>%
group_by(Subj) %>%
summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))
VB_acc <- aida::bootstrapped_CI(d_VB$Accuracy)
VB_trials <- aida::bootstrapped_CI(d_VB$Trials)
VB_ca <- aida::bootstrapped_CI(d_VB$CA)
NV_acc <- aida::bootstrapped_CI(d_NV$Accuracy)
NV_trials <- aida::bootstrapped_CI(d_NV$Trials)
NV_ca <- aida::bootstrapped_CI(d_NV$CA)
measure <- c("Trials", "Trials", "Accuracy", "Accuracy", "CA", "CA")
cond <- c("NV", "VB","NV", "VB","NV", "VB")
summary_stats <- full_join(NV_trials, VB_trials) %>%
full_join(., NV_acc) %>%
full_join(., VB_acc) %>%
full_join(., NV_ca) %>%
full_join(., VB_ca) %>%
mutate(measure = measure, Cond = cond) %>%
select(measure, Cond, lower, mean, upper)
summary_stats
d_unique <- d_tidy %>%
group_by(Subj) %>%
distinct(Subj, .keep_all = TRUE) %>%
select(Subj, Cond, Trials, Accuracy, CA)
bins_trials <- d_unique["Trials"] %>% unique %>% nrow
d_unique %>%
ggplot(aes(x = Trials)) +
# TODO: binwidth or bins? Also, bins_trials * 2 or just bins_trials?
geom_histogram(bins=bins_trials) +
facet_wrap(Cond ~ .)
bins_acc <- d_unique["Accuracy"] %>% unique %>% nrow
d_unique %>%
ggplot(aes(x = Accuracy)) +
# TODO: binwidth or bins? Also, bins_acc * 2 or just bins_acc?
geom_histogram(bins=bins_acc) +
facet_wrap(Cond ~ .)
bins_ca <- d_unique["CA"] %>% unique %>% nrow
d_unique %>%
ggplot(aes(x = CA)) +
# TODO: binwidth or bins? Also, bins_ca * 2 or just bins_ca?
geom_histogram(bins=bins_ca) +
facet_wrap(Cond ~ .)
d_unique <- d_unique %>%
mutate(
Cond = factor(Cond)
)
fit_ls <- brm(
formula = Trials ~ Cond,
data = d_unique,
prior = prior(student_t(1, 0, 100),  class = 'b'),
sample_prior = 'yes',
iter = 20000
)
(summary(fit_ls))
# mu_NV > mu_VB is our hypothesis
brms::hypothesis(fit_ls, "CondNV > 0")
d_tidy <- d_tidy %>%
mutate(
Cond = factor(Cond)
)
fit_acc <- brm(
formula = Accuracy ~ Cond,
data = d_tidy,
family = bernoulli(link = "logit"),
prior = prior(student_t(1, 0, 100), class='b'),
sample_prior = 'yes',
iter = 20000
)
View(d_tidy)
knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)
# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)
# package for this course
library(aida)
# package for Bayesian regression
library(brms)
# parallel execution of Stan code
options(mc.cores = parallel::detectCores())
# use the aida-theme for plotting
theme_set(theme_aida())
# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
# setting theme colors globally
scale_colour_discrete <- function(...) {
scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
scale_fill_manual(..., values = project_colors)
}
# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
cache = TRUE, fig.align = 'center')
d <- read_csv('BM_data.csv')
num_participants <- d %>%
group_by(Subj) %>%
nrow
num_condition <- d %>%
group_by(Cond) %>%
count()
num_NV <- num_condition[1,2]
num_VB <- num_condition[2,2]
d_tidy <- d %>%
select(., -seq(26,35)) %>%
pivot_longer(., cols = seq(6,25), names_to = "trial_label", values_to = "response")
d_tidy
# TODO check for which decision the CA strategy would make for 1-5 and 6-10 respectively (brain tired)
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(trial_type = case_when(
str_detect(trial_label, "A") ~ "category_A",
str_detect(trial_label, "B") ~ "category_B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "exception_2",
str_detect(trial_label, "T[1-5]") ~ "exception_1",
str_detect(trial_label, "T[6-9]") ~ "exception_2",
TRUE ~ "error"
))
d_tidy
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(expected_response = case_when(
str_detect(trial_label, "A") ~ "A",
str_detect(trial_label, "B") ~ "B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "A",
str_detect(trial_label, "T[1-5]") ~ "B",
str_detect(trial_label, "T[6-9]") ~ "A",
TRUE ~ "error"
))
d_ca_check <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]")) %>%
group_by(Subj) %>%
mutate(CA_check = mean(as.character(response) == as.character(expected_response)))
d_accuracy_check <- d_tidy %>%
filter(str_detect(trial_label, "[AB]")) %>%
group_by(Subj) %>%
mutate(Accuracy_check = mean(as.character(response) == as.character(expected_response)))
d_ca_check
d_accuracy_check
all(d_ca_check["CA_check"] == d_ca_check["CA"])
all(d_accuracy_check["Accuracy_check"] == d_accuracy_check["Accuracy"])
# your output should conform to this form-template
tribble(
~"measure", ~"Cond", ~"lower", ~"mean", ~"upper",
"Trials", "NV", "...", "...", "...",
"Trials", "VB", "...", "...", "...",
"Accuracy", "NV", "...", "...", "...",
"Accuracy", "VB", "...", "...", "...",
"CA", "NV", "...", "...", "...",
"CA", "VB", "...", "...", "..."
)
# Solution:
d_VB <- d_tidy %>%
filter(Cond == "VB") %>%
group_by(Subj) %>%
summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))
d_NV <- d_tidy %>%
filter(Cond == "NV") %>%
group_by(Subj) %>%
summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))
VB_acc <- aida::bootstrapped_CI(d_VB$Accuracy)
VB_trials <- aida::bootstrapped_CI(d_VB$Trials)
VB_ca <- aida::bootstrapped_CI(d_VB$CA)
NV_acc <- aida::bootstrapped_CI(d_NV$Accuracy)
NV_trials <- aida::bootstrapped_CI(d_NV$Trials)
NV_ca <- aida::bootstrapped_CI(d_NV$CA)
measure <- c("Trials", "Trials", "Accuracy", "Accuracy", "CA", "CA")
cond <- c("NV", "VB","NV", "VB","NV", "VB")
summary_stats <- full_join(NV_trials, VB_trials) %>%
full_join(., NV_acc) %>%
full_join(., VB_acc) %>%
full_join(., NV_ca) %>%
full_join(., VB_ca) %>%
mutate(measure = measure, Cond = cond) %>%
select(measure, Cond, lower, mean, upper)
summary_stats
d_unique <- d_tidy %>%
group_by(Subj) %>%
distinct(Subj, .keep_all = TRUE) %>%
select(Subj, Cond, Trials, Accuracy, CA)
bins_trials <- d_unique["Trials"] %>% unique %>% nrow
d_unique %>%
ggplot(aes(x = Trials)) +
# TODO: binwidth or bins? Also, bins_trials * 2 or just bins_trials?
geom_histogram(bins=bins_trials) +
facet_wrap(Cond ~ .)
bins_acc <- d_unique["Accuracy"] %>% unique %>% nrow
d_unique %>%
ggplot(aes(x = Accuracy)) +
# TODO: binwidth or bins? Also, bins_acc * 2 or just bins_acc?
geom_histogram(bins=bins_acc) +
facet_wrap(Cond ~ .)
bins_ca <- d_unique["CA"] %>% unique %>% nrow
d_unique %>%
ggplot(aes(x = CA)) +
# TODO: binwidth or bins? Also, bins_ca * 2 or just bins_ca?
geom_histogram(bins=bins_ca) +
facet_wrap(Cond ~ .)
d_unique <- d_unique %>%
mutate(
Cond = factor(Cond)
)
fit_ls <- brm(
formula = Trials ~ Cond,
data = d_unique,
prior = prior(student_t(1, 0, 100),  class = 'b'),
sample_prior = 'yes',
# TODO: change back to 20k
iter = 1000
)
(summary(fit_ls))
# mu_NV > mu_VB is our hypothesis
brms::hypothesis(fit_ls, "CondNV > 0")
d_acc <- d_tidy %>%
filter(str_detect(trial_label, "[AB]")) %>%
group_by(Subj) %>%
mutate(correct = response == expected_response) %>%
mutate(Cond = factor(Cond))
fit_acc <- brm(
formula = correct ~ Cond,
data = d_acc,
family = bernoulli(link = "logit"),
prior = prior(student_t(1, 0, 100), class='b'),
sample_prior = 'yes',
# TODO: change back to 20k
iter = 1000
)
knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)
d_tidy
knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)
# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)
# package for this course
library(aida)
knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)
# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)
# package for this course
library(aida)
# package for Bayesian regression
library(brms)
# parallel execution of Stan code
options(mc.cores = parallel::detectCores())
# use the aida-theme for plotting
theme_set(theme_aida())
# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
# setting theme colors globally
scale_colour_discrete <- function(...) {
scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
scale_fill_manual(..., values = project_colors)
}
# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
cache = TRUE, fig.align = 'center')
d <- read_csv('BM_data.csv')
num_participants <- d %>%
group_by(Subj) %>%
nrow
num_condition <- d %>%
group_by(Cond) %>%
count()
num_NV <- num_condition[1,2]
num_VB <- num_condition[2,2]
d_tidy <- d %>%
select(., -seq(26,35)) %>%
pivot_longer(., cols = seq(6,25), names_to = "trial_label", values_to = "response")
d_tidy
# TODO check for which decision the CA strategy would make for 1-5 and 6-10 respectively (brain tired)
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(trial_type = case_when(
str_detect(trial_label, "A") ~ "category_A",
str_detect(trial_label, "B") ~ "category_B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "exception_2",
str_detect(trial_label, "T[1-5]") ~ "exception_1",
str_detect(trial_label, "T[6-9]") ~ "exception_2",
TRUE ~ "error"
))
d_tidy
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(expected_response = case_when(
str_detect(trial_label, "A") ~ "A",
str_detect(trial_label, "B") ~ "B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "A",
str_detect(trial_label, "T[1-5]") ~ "B",
str_detect(trial_label, "T[6-9]") ~ "A",
TRUE ~ "error"
))
d_ca_check <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]")) %>%
group_by(Subj) %>%
mutate(CA_check = mean(as.character(response) == as.character(expected_response)))
d_accuracy_check <- d_tidy %>%
filter(str_detect(trial_label, "[AB]")) %>%
group_by(Subj) %>%
mutate(Accuracy_check = mean(as.character(response) == as.character(expected_response)))
d_ca_check
d_accuracy_check
all(d_ca_check["CA_check"] == d_ca_check["CA"])
all(d_accuracy_check["Accuracy_check"] == d_accuracy_check["Accuracy"])
d_tidy
# number of trials in which decision was based on CA strategy
k <-
# total number of trials
N <- nrow(d_tidy)
N
d_tidy
d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
nrow(., as.character(response) == as.character(expected_response))
d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
nrow(filter(., as.character(response) == as.character(expected_response)))
d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
(filter(., as.character(response) == as.character(expected_response))
k
d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
nrow(filter(., as.character(response) == as.character(expected_response))))
d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response))
k
# total number of trials
N <- nrow(d_tidy)
N
#d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response))
k
# total number of trials
N <- nrow(d_tidy)
N
#d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response)) %>%
nrow()
k
# total number of trials
N <- nrow(d_tidy)
N
#d_tidy
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response)) %>%
nrow()
k
# number of test trials
N <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]"))%>%
nrow()
N
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response)) %>%
nrow()
# number of test trials
N <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]"))%>%
nrow()
samples <- rbeta(1e6, 457, 225)
aida::summarize_sample_vector(samples)
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response)) %>%
nrow()
# number of test trials
N <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]"))%>%
nrow()
samples <- rbeta(1e6, 457, 225)
aida::summarize_sample_vector(samples, "bias towards CA")
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response)) %>%
nrow()
k
# number of test trials
N <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]"))%>%
nrow()
# number of trials in which decision was based on CA strategy
k <-  d_tidy %>%
# only look at test trials in which we can determine used strategy
filter(!str_detect(trial_label, "[AB]")) %>%
# count number of trials in which the response is the same as the expected response, assuming CA strategy
filter(., as.character(response) == as.character(expected_response)) %>%
nrow()
k
# number of test trials
N <- d_tidy %>%
filter(!str_detect(trial_label, "[AB]"))%>%
nrow()
N
