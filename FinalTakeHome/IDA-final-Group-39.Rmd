---
title: "Final take-home exam"
date: 'Due: Wednesday, February 10 by 08:00 pm CET'
author: ""
output: 
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)

```

```{r libraries, include=FALSE, message=FALSE, warning=FALSE}

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for this course
library(aida)

# package for Bayesian regression
library(brms)

# parallel execution of Stan code
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
} 

# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      cache = TRUE, fig.align = 'center')
```

# Instructions

* Create an Rmd-file with your group number (equivalent to StudIP group) in the 'author' heading and answer the following questions.
* When all answers are ready, 'Knit' the document to produce a HTML file.
* Create a ZIP archive called "IDA-final-Group-XYZ.zip" (where 'XYZ' is *your* group number) containing:
   * an R Markdown file "IDA-final-Group-XYZ.Rmd"
   * a knitted HTML document "IDA-final-Group-XYZ.html"
   * any other files necessary to compile your Rmarkdown to HTML (data, pictures etc.)
* Upload the ZIP archive on Stud.IP in your group folder before the deadline (see above). You may upload as many times as you like before the deadline, only your final submission will count.
* If you run `brms::brm` in an R code chunk, make sure to include `results='hide'` as an option to that code chunk to suppress the output of Stan.
* If you have questions, pose them on StudIP in the new section for questions on the exam. Obviously, do **not** post solutions. 

# <span style = "color:firebrick">Exercise 1:</span> Identify hypotheses & relevant measures [8 points]

Read the paper by Brashears & Minda (2020) carefully.
(Authors/paper henceforth referred to as B&M.)
The focus for us is on the sections "Methods" and "Results".

## Ex 1.a Identify the research hypotheses [4 points]

B&M address three main research hypotheses, and they conduct three main statistical analyses, one for each of three relevant research hypotheses.
We will refer to these research hypotheses here as "Learning Speed", "Accuracy" and "Strategy".

Here's a formulation of the "Strategy" hypothesis:

  > "Strategy": The proportion of categorization choices for each subject, which are indicative of a 'critical attribute' categorization strategy (as stored in variable `CA` and derived from choices in the critical trials `T1` ... `T10`), is larger for the "easily verbalizable" condition (level `VB` of variable `Cond`), than in the "not-easily verbalizable" condition (level `NB` of variable `Cond`).

Formulate the remaining two hypotheses in natural language as precisely as possible, using -wherever possible- reference to the names and/or values of variables in the data set supplied by B&M, following the above example.
(That is, if some hypothesis relates to a dependent or independent variable, then add the information of where in the data set this variable is stored, and/or which values of it you are referring to precisely.)

Solution: 
Private: Easily verbalized items will be learned faster and retained better in memory.
> "Learning Speed": Private: The learning speed for the easily verbalized condition is higher than for the not-easily verbalized condition.
The number of trials before making a correct categorization in 12 trials in a row (minimum 60 previous trials, variable 'Trials' TODO) is smaller for the "easily verbalize" condition (level 'VB' of variable 'Cond') than for the "not-easily verbalize" condition (level 'NB' of variable 'Cond').
> "Accuracy": Private: Accuracy is higher for the easily verbalized condition than for the not-easily verbalized condition.
The rate of accuracy on the training items in the test phase (variable 'Accuracy') is higher for "easily verbalize" condition (level 'VB' of variable 'Cond') than for the "not-easily verbalize" condition (level 'NB' of variable 'Cond').

## Ex 1.b Previous results [3 points]

What results do B&M report for these three hypotheses?
Make a short statement for each hypothesis, e.g., "find evidence for/against" or "find no evidence", for each hypothesis.
(Don't overthink this! The question is about what the authors conclude from their analyses. No need to go down the rabbit hole here and triple-check whether you would draw similar conclusions from the reported analyses.)

Solution: TODO include measures from t-Tests?
> "Learning Speed": Learning rate for 'easily verbalize' condition was not significantly different from learning rate in 'not-easily verbalize' condition, so evidence against the hypothesis. 
> "Accuracy": Accuracy was significantly higher in 'easily verbalize' condition than in 'not-easily verbalized condition', so evidence in favor of the hypothesis. 
> "Strategy": Which hypothesis are we supposed to compare to? 
Paper: easily verbalized condition -> preference for CA, not-easily verbalized condition -> preference for FR
Task: easily verbalized condition -> some preference, not easily verbalized condition -> less preference for CA than the other preference

## Ex 1.c Kind of experiment [1 point]

What kind of experiment is this? I.e., which independent (explanatory) variables are there? Are they metric or categorical factors? If factors, how many levels (ordered, unordered?) do they have? Any within- or between-subjects manipulations?

Solution: In this experiment, the influence of verbalizability of features (independent variable) on learning speed, accuracy and learning strategy (dependent variables) is tested. 
Learning speed is a metric factor, which ranges from at least 60 to 400?
Accuracy is a metric factor ranging from 0-1.

Solution:
> independent variables
: critical attribute(CA) vs family resemblance(FR)
: easily-verbalizable-features vs not-easily-verbalizable features

> categorical factors

> unordered

> within-subject for the CA vs FR independent variable
  between-subject for the easily-verbalizable-features vs not-easily-verbalizable features
# <span style = "color:firebrick">Exercise 2:</span> Inspect the data [12 points]

We load the data:

```{r}
d <- read_csv('BM_data.csv')
```


## Ex 2.a Get some counts [2 points]

Use R to retrieve:

- the number of participants contained in the data set
- the number of times each condition (NV and VB) occurs in the data
```{r}
num_participants <- d %>% 
  group_by(Subj) %>% 
  nrow

num_condition <- d %>% 
  group_by(Cond) %>% 
  count()
num_NV <- num_condition[1,2]
num_VB <- num_condition[2,2]
```


## Ex 2.b Tidy up Part 1 [2 points]

In some of our subsequent analyses, we will be interested in the categorization decision of each subject for each critical experimental trial.
For this purpose, the data in `d` is not tidy.
Use common `dplyr` functions to create a new variable `d_tidy` which contains the same data as `d` (except for data from T11-T20) in a tidy format.
So, `d_tidy` will include one row for each individual categorization trial of each subject.
Since we will not be analyzing the "single-feature trials" (T11-T20), exclude these.
Name the two new columns you will create by pivoting `trial_label` and `response`.
Include the first 10 lines of your `d_tidy` to the HTML output. 

```{r}
d_tidy <- d %>%
  select(., -seq(26,35)) %>%
  pivot_longer(., cols = seq(6,25), names_to = "trial_label", values_to = "response") 
d_tidy
```


## Ex 2.c Tidy up Part 2 [2 points]

The tibble in `d_tidy` now contains a data column with entries like `TA1` or `T10`, but actually `TA1` and `T10` are quite different types of trials.
So, let's clean up and structure some more.
Introduce a new column called `trial_type`, which contains the information whether each row's trial is from one of the four categories: `category_A`, `category_B`, `exception_1`, or `exception_2`.
The trials of types `exception_1` and `exception_2` differ with respect to the expected response under the "characteristic attribute" (CA) strategy.
Trials of type `exception_1` expect response "B" under CA.
Add also a column called `expected_repsonse` containing the expected responses for each trial type (assuming strategy CA, where that is relevant).

-> look at table of T1-10 configurations, evaluate which category CA would choose -> add column -> expected B = exception_1, expected A = exception_2

```{r} 
# TODO check for which decision the CA strategy would make for 1-5 and 6-10 respectively (brain tired)
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(trial_type = case_when(
str_detect(trial_label, "A") ~ "category_A",
str_detect(trial_label, "B") ~ "category_B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "exception_2",
str_detect(trial_label, "T[1-5]") ~ "exception_1",
str_detect(trial_label, "T[6-9]") ~ "exception_2",
TRUE ~ "error"
))
d_tidy

d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(expected_response = case_when(
str_detect(trial_label, "A") ~ "A",
str_detect(trial_label, "B") ~ "B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "A",
str_detect(trial_label, "T[1-5]") ~ "B",
str_detect(trial_label, "T[6-9]") ~ "A",
TRUE ~ "error"
))
```


## Ex 2.d Double-check relevant summary stats by B&M [3 points]

Checking other researchers' results is not imprudent, but part of a healthy self-correcting and cooperative scientific community.
Therefore, add to the tibble in `d` columns `CA_check` and `Accuracy_check`, calculating these values yourself from the information stored in columns `trial_type` and `expected_response` of `d_tidy`.
(NB: you may want to use some form of 'joining' tibbles, or you just produce two single vectors `CA_check` and `Accuracy_check`.)
Finally, use R (e.g., functions like `all` or `any`) to check if your calculations agree the information provided in columns `CA` and `Accuracy` of `d`.
```{r}
d_ca_check <- d_tidy %>%
  filter(!str_detect(trial_label, "[AB]")) %>%
  group_by(Subj) %>% 
  mutate(CA_check = mean(as.character(response) == as.character(expected_response)))
d_accuracy_check <- d_tidy %>%
    filter(str_detect(trial_label, "[AB]")) %>%
  group_by(Subj) %>% 
  mutate(Accuracy_check = mean(as.character(response) == as.character(expected_response)))

d_ca_check
d_accuracy_check

all(d_ca_check["CA_check"] == d_ca_check["CA"])
all(d_accuracy_check["Accuracy_check"] == d_accuracy_check["Accuracy"])
```


## Ex 2.e Summary statistics [3 points]

Use common `dplyr` functions, as well as the function `aida::bootstrapped_CI` to produce a tibble with the exact form as shown below (including ordering of rows
and columns, as well as column names!) showing the mean, the lower- and the upper-bound of the 95% bootstrapped confidence interval of the mean (as returned by `aida::bootstrapped_CI`) of the three relevant measures of interest. 


```{r, echo = F}
# your output should conform to this form-template
tribble(
  ~"measure", ~"Cond", ~"lower", ~"mean", ~"upper",
  "Trials", "NV", "...", "...", "...",
  "Trials", "VB", "...", "...", "...",
  "Accuracy", "NV", "...", "...", "...",
  "Accuracy", "VB", "...", "...", "...",
  "CA", "NV", "...", "...", "...",
  "CA", "VB", "...", "...", "..."
)

# Solution:
d_VB <- d_tidy %>% 
  filter(Cond == "VB") %>% 
  group_by(Subj) %>% 
  summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))

d_NV <- d_tidy %>% 
  filter(Cond == "NV") %>% 
  group_by(Subj) %>% 
  summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))

VB_acc <- aida::bootstrapped_CI(d_VB$Accuracy)
VB_trials <- aida::bootstrapped_CI(d_VB$Trials)
VB_ca <- aida::bootstrapped_CI(d_VB$CA)
NV_acc <- aida::bootstrapped_CI(d_NV$Accuracy)
NV_trials <- aida::bootstrapped_CI(d_NV$Trials)
NV_ca <- aida::bootstrapped_CI(d_NV$CA)

measure <- c("Trials", "Trials", "Accuracy", "Accuracy", "CA", "CA") 
cond <- c("NV", "VB","NV", "VB","NV", "VB")

summary_stats <- full_join(NV_trials, VB_trials) %>% 
  full_join(., NV_acc) %>% 
  full_join(., VB_acc) %>% 
  full_join(., NV_ca) %>% 
  full_join(., VB_ca) %>%
  mutate(measure = measure, Cond = cond) %>%
  select(measure, Cond, lower, mean, upper)

summary_stats

```

# <span style = "color:firebrick">Exercise 3:</span> Vizualization [7 points]

We will look at plots, one for each of the three main research hypotheses.
For your own understanding, think about whether you can "see evidence" in favor of or against each hypothesis.
(You do not have to comment on this in your submission.)

## Ex 3.a Vizualize the "Learning Speed" data [3 points]

Visualize the data relevant for addressing the "Learning Speed" hypothesis.
Use a histogram with the most fine-grained binning which is reasonable given the nature of the data (i.e., manipulate argument `binwidth` in function `geom_histogram`).
Use two facets, one for each relevant level of the categorical predictor.

## Ex 3.b Vizualize the "Accuracy" data [2 points]

Make a histogram plot, using facets, for the data relevant to the "Accuracy" hypothesis.

## Ex 3.c Vizualize the "Strategy" data [2 points]

Plot the data for the "Strategy" hypothesis.
You can use a histogram and facets here as well.

# <span style = "color:firebrick">Exercise 4:</span> Hypothesis testing [22 points]

## Ex 4.a Testing the "Learning Speed" hypothesis [6 points]

Test the "Learning Speed" hypothesis using a Bayesian linear regression model, following the assumption made in the paper that the measures in `Trials` are normally distributed.
Make sure to also record samples from the prior (even if we don't need them), and set the relevant prior for the slope term to a Student's $t$ distribution with 1 degree of freedom, mean 0 and standard deviation 100.
Test the hypothesis with `brms::hypothesis` and interpret the outcome with explicit mentioning of the quantities you base your interpretation on.

## Ex 4.b Testing the "Accuracy" hypothesis [8 points]

B&M test the "Accuracy" hypothesis based on the assumption that the measures in `Accuracy` are normally distributed. 
If you can, explain in one concise sentence why this normality assumption might be criticized in the case at hand.
If you cannot think of a different analysis you like better, follow this strategy to test the "Accuracy" hypothesis with a Bayesian regression model following the same general approach as in the previous exercise.
If you can think of an analysis you might prefer, do that (for 2 extra points).
Use the same priors as in the previous exercise.

## Ex 4.c Testing the "Strategy" hypothesis [8 points]

Test the "Strategy" hypothesis in the way that you think is best appropriate (but using a method that we have covered in class).

# <span style = "color:firebrick">Exercise 5 (challenge question):</span> Comparison to ANOVA analysis [5 points]

B&M's ANOVA-based analysis for the "Strategy" Hypothesis actually also tested a fourth hypothesis, for which a significant test result was obtained.
Describe in words what that finding was.
Use conjugacy (starting from flat priors) to quickly sample from a Bayesian posterior to test this additional hypothesis using Kruschke's ternary logic for hypothesis testing.
(**Hint:** Did you really think you could take this final w/o addressing an issue translatable into a question about whether some stupid coin is fair or biased on some way or other?)

**Solution:**

The reported main effect of category amounts to saying that participants seem to have favored choices indicative of a "characterizing attribute" (CA) strategy. 
