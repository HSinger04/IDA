---
title: "Final take-home exam"
date: 'Due: Wednesday, February 10 by 08:00 pm CET'
author: ""
output: 
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)

```

```{r libraries, include=FALSE, message=FALSE, warning=FALSE}

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for this course
library(aida)

# package for Bayesian regression
library(brms)

# parallel execution of Stan code
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
} 

# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      cache = TRUE, fig.align = 'center')
```

# Instructions

* Create an Rmd-file with your group number (equivalent to StudIP group) in the 'author' heading and answer the following questions.
* When all answers are ready, 'Knit' the document to produce a HTML file.
* Create a ZIP archive called "IDA-final-Group-XYZ.zip" (where 'XYZ' is *your* group number) containing:
   * an R Markdown file "IDA-final-Group-XYZ.Rmd"
   * a knitted HTML document "IDA-final-Group-XYZ.html"
   * any other files necessary to compile your Rmarkdown to HTML (data, pictures etc.)
* Upload the ZIP archive on Stud.IP in your group folder before the deadline (see above). You may upload as many times as you like before the deadline, only your final submission will count.
* If you run `brms::brm` in an R code chunk, make sure to include `results='hide'` as an option to that code chunk to suppress the output of Stan.
* If you have questions, pose them on StudIP in the new section for questions on the exam. Obviously, do **not** post solutions. 

# <span style = "color:firebrick">Exercise 1:</span> Identify hypotheses & relevant measures [8 points]

Read the paper by Brashears & Minda (2020) carefully.
(Authors/paper henceforth referred to as B&M.)
The focus for us is on the sections "Methods" and "Results".

## Ex 1.a Identify the research hypotheses [4 points]

B&M address three main research hypotheses, and they conduct three main statistical analyses, one for each of three relevant research hypotheses.
We will refer to these research hypotheses here as "Learning Speed", "Accuracy" and "Strategy".

Here's a formulation of the "Strategy" hypothesis:

  > "Strategy": The proportion of categorization choices for each subject, which are indicative of a 'critical attribute' categorization strategy (as stored in variable `CA` and derived from choices in the critical trials `T1` ... `T10`), is larger for the "easily verbalizable" condition (level `VB` of variable `Cond`), than in the "not-easily verbalizable" condition (level `NB` of variable `Cond`).

Formulate the remaining two hypotheses in natural language as precisely as possible, using -wherever possible- reference to the names and/or values of variables in the data set supplied by B&M, following the above example.
(That is, if some hypothesis relates to a dependent or independent variable, then add the information of where in the data set this variable is stored, and/or which values of it you are referring to precisely.)

Solution: 

Private: Easily verbalized items will be learned faster and retained better in memory.

  > "Learning Speed": Private: The learning speed for the easily verbalized condition is higher than for the not-easily verbalized condition. 
The number of trials before making a correct categorization in 12 trials in a row (minimum 60 trials) (as stored in variable `Trials` \# TODO: unclear if TA1-TB5 should be mentioned
or not, since they are for testing, not training AFAIK) is smaller for the "easily verbalize" condition (level `VB` of variable `Cond`) than for the "not-easily verbalize" condition (level `NB` of variable `Cond`).


  > "Accuracy": Private: Accuracy is higher for the easily verbalized condition than for the not-easily verbalized condition.
The proportion of correctly classifying the training items in the test phase (as stored in variable `Accuracy` and derived from choices in the critical trials `TA1` ... `TA5` and `TB1` ... `TB5`) is higher for "easily verbalize" condition (level `VB` of variable `Cond`) than for the "not-easily verbalize" condition (level `NB` of variable `Cond`).

## Ex 1.b Previous results [3 points]

What results do B&M report for these three hypotheses?
Make a short statement for each hypothesis, e.g., "find evidence for/against" or "find no evidence", for each hypothesis.
(Don't overthink this! The question is about what the authors conclude from their analyses. No need to go down the rabbit hole here and triple-check whether you would draw similar conclusions from the reported analyses.)

Solution: TODO include measures from t-Tests?
  > "Learning Speed": Learning rate for the 'easily verbalize' condition and the 'not-easily verbalize' condition did not differ significantly from one another, so evidence against the hypothesis. 
  > "Accuracy": Accuracy was significantly higher in 'easily verbalize' condition than in 'not-easily verbalized condition', so evidence in favor of the hypothesis. 
  > "Strategy": \# TODO: Task: easily verbalized condition -> some preference, not easily verbalized condition -> less preference for CA than the other preference

## Ex 1.c Kind of experiment [1 point]

What kind of experiment is this? I.e., which independent (explanatory) variables are there? Are they metric or categorical factors? If factors, how many levels (ordered, unordered?) do they have? Any within- or between-subjects manipulations?

Solution: In this experiment, the influence of verbalizability of features (independent variable) on learning speed, accuracy and learning strategy (dependent variables) is tested. 
Learning speed is a metric factor, which ranges from at least 60 to 400.
\# TODO: Are CA, FR and Accuracy actually considered metric? They are generated
from a bernoulli dist.
Accuracy, CA and FR are a metric factors ranging from 0-1 in steps of 0.1. The relationship between CA and FR for a subject is as follows: FR = 1 - CA.
\# TODO

Solution:
> independent variables
: critical attribute(CA) vs family resemblance(FR)
: easily-verbalizable-features vs not-easily-verbalizable features

> categorical factors

> unordered

> within-subject for the CA vs FR independent variable
  between-subject for the easily-verbalizable-features vs not-easily-verbalizable features
# <span style = "color:firebrick">Exercise 2:</span> Inspect the data [12 points]

We load the data:

```{r}
d <- read_csv('BM_data.csv')
```


## Ex 2.a Get some counts [2 points]

Use R to retrieve:

- the number of participants contained in the data set
- the number of times each condition (NV and VB) occurs in the data
```{r}
num_participants <- d %>% 
  group_by(Subj) %>% 
  nrow

num_condition <- d %>% 
  group_by(Cond) %>% 
  count()
num_NV <- num_condition[1,2]
num_VB <- num_condition[2,2]
num_condition
```


## Ex 2.b Tidy up Part 1 [2 points]

In some of our subsequent analyses, we will be interested in the categorization decision of each subject for each critical experimental trial.
For this purpose, the data in `d` is not tidy.
Use common `dplyr` functions to create a new variable `d_tidy` which contains the same data as `d` (except for data from T11-T20) in a tidy format.
So, `d_tidy` will include one row for each individual categorization trial of each subject.
Since we will not be analyzing the "single-feature trials" (T11-T20), exclude these.
Name the two new columns you will create by pivoting `trial_label` and `response`.
Include the first 10 lines of your `d_tidy` to the HTML output. 

```{r}
d_tidy <- d %>%
  select(., -seq(26,35)) %>%
  pivot_longer(., cols = seq(6,25), names_to = "trial_label", values_to = "response") 
d_tidy
```


## Ex 2.c Tidy up Part 2 [2 points]

The tibble in `d_tidy` now contains a data column with entries like `TA1` or `T10`, but actually `TA1` and `T10` are quite different types of trials.
So, let's clean up and structure some more.
Introduce a new column called `trial_type`, which contains the information whether each row's trial is from one of the four categories: `category_A`, `category_B`, `exception_1`, or `exception_2`.
The trials of types `exception_1` and `exception_2` differ with respect to the expected response under the "characteristic attribute" (CA) strategy.
Trials of type `exception_1` expect response "B" under CA.
Add also a column called `expected_repsonse` containing the expected responses for each trial type (assuming strategy CA, where that is relevant).

```{r} 
# TODO check for which decision the CA strategy would make for 1-5 and 6-10 respectively (brain tired)
d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(trial_type = case_when(
str_detect(trial_label, "A") ~ "category_A",
str_detect(trial_label, "B") ~ "category_B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "exception_2",
str_detect(trial_label, "T[1-5]") ~ "exception_1",
str_detect(trial_label, "T[6-9]") ~ "exception_2",
TRUE ~ "error"
))
d_tidy

d_tidy <- d_tidy %>% group_by(trial_label) %>% mutate(expected_response = case_when(
str_detect(trial_label, "A") ~ "A",
str_detect(trial_label, "B") ~ "B",
# needs to come before "T[1-5]"
str_detect(trial_label, "T10") ~ "A",
str_detect(trial_label, "T[1-5]") ~ "B",
str_detect(trial_label, "T[6-9]") ~ "A",
TRUE ~ "error"
))
```


## Ex 2.d Double-check relevant summary stats by B&M [3 points]

Checking other researchers' results is not imprudent, but part of a healthy self-correcting and cooperative scientific community.
Therefore, add to the tibble in `d` columns `CA_check` and `Accuracy_check`, calculating these values yourself from the information stored in columns `trial_type` and `expected_response` of `d_tidy`.
(NB: you may want to use some form of 'joining' tibbles, or you just produce two single vectors `CA_check` and `Accuracy_check`.)
Finally, use R (e.g., functions like `all` or `any`) to check if your calculations agree the information provided in columns `CA` and `Accuracy` of `d`.
```{r}
d_ca_check <- d_tidy %>%
  filter(!str_detect(trial_label, "[AB]")) %>%
  group_by(Subj) %>% 
  mutate(CA_check = mean(as.character(response) == as.character(expected_response)))
d_accuracy_check <- d_tidy %>%
    filter(str_detect(trial_label, "[AB]")) %>%
  group_by(Subj) %>% 
  mutate(Accuracy_check = mean(as.character(response) == as.character(expected_response)))

d_ca_check
d_accuracy_check

all(d_ca_check["CA_check"] == d_ca_check["CA"])
all(d_accuracy_check["Accuracy_check"] == d_accuracy_check["Accuracy"])
```


## Ex 2.e Summary statistics [3 points]

Use common `dplyr` functions, as well as the function `aida::bootstrapped_CI` to produce a tibble with the exact form as shown below (including ordering of rows
and columns, as well as column names!) showing the mean, the lower- and the upper-bound of the 95% bootstrapped confidence interval of the mean (as returned by `aida::bootstrapped_CI`) of the three relevant measures of interest. 


```{r, echo = F}
# your output should conform to this form-template
tribble(
  ~"measure", ~"Cond", ~"lower", ~"mean", ~"upper",
  "Trials", "NV", "...", "...", "...",
  "Trials", "VB", "...", "...", "...",
  "Accuracy", "NV", "...", "...", "...",
  "Accuracy", "VB", "...", "...", "...",
  "CA", "NV", "...", "...", "...",
  "CA", "VB", "...", "...", "..."
)

# Solution:
d_VB <- d_tidy %>% 
  filter(Cond == "VB") %>% 
  group_by(Subj) %>% 
  summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))

d_NV <- d_tidy %>% 
  filter(Cond == "NV") %>% 
  group_by(Subj) %>% 
  summarise(Trials = mean(Trials), Accuracy = mean(Accuracy), CA = mean(CA))

VB_acc <- aida::bootstrapped_CI(d_VB$Accuracy)
VB_trials <- aida::bootstrapped_CI(d_VB$Trials)
VB_ca <- aida::bootstrapped_CI(d_VB$CA)
NV_acc <- aida::bootstrapped_CI(d_NV$Accuracy)
NV_trials <- aida::bootstrapped_CI(d_NV$Trials)
NV_ca <- aida::bootstrapped_CI(d_NV$CA)

measure <- c("Trials", "Trials", "Accuracy", "Accuracy", "CA", "CA") 
cond <- c("NV", "VB","NV", "VB","NV", "VB")

summary_stats <- full_join(NV_trials, VB_trials) %>% 
  full_join(., NV_acc) %>% 
  full_join(., VB_acc) %>% 
  full_join(., NV_ca) %>% 
  full_join(., VB_ca) %>%
  mutate(measure = measure, Cond = cond) %>%
  select(measure, Cond, lower, mean, upper)

summary_stats
d_tidy

```

# <span style = "color:firebrick">Exercise 3:</span> Vizualization [7 points]

We will look at plots, one for each of the three main research hypotheses.
For your own understanding, think about whether you can "see evidence" in favor of or against each hypothesis.
(You do not have to comment on this in your submission.)

## Ex 3.a Vizualize the "Learning Speed" data [3 points]

Visualize the data relevant for addressing the "Learning Speed" hypothesis.
Use a histogram with the most fine-grained binning which is reasonable given the nature of the data (i.e., manipulate argument `binwidth` in function `geom_histogram`).
Use two facets, one for each relevant level of the categorical predictor.

```{r}
d_unique <- d_tidy %>% 
  group_by(Subj) %>% 
  distinct(Subj, .keep_all = TRUE) %>%
   select(Subj, Cond, Trials, Accuracy, CA)

bins_trials <- d_unique["Trials"] %>% unique %>% nrow

d_unique %>% 
  ggplot(aes(x = Trials)) +
  # TODO: binwidth or bins? Also, bins_trials * 2 or just bins_trials?
  geom_histogram(bins=bins_trials) +
  facet_wrap(Cond ~ .)
```

## Ex 3.b Vizualize the "Accuracy" data [2 points]

Make a histogram plot, using facets, for the data relevant to the "Accuracy" hypothesis.

```{r}
bins_acc <- d_unique["Accuracy"] %>% unique %>% nrow

d_unique %>% 
  ggplot(aes(x = Accuracy)) +
  # TODO: binwidth or bins? Also, bins_acc * 2 or just bins_acc?
  geom_histogram(bins=bins_acc) +
  facet_wrap(Cond ~ .)
```

## Ex 3.c Vizualize the "Strategy" data [2 points]

Plot the data for the "Strategy" hypothesis.
You can use a histogram and facets here as well.

```{r}
bins_ca <- d_unique["CA"] %>% unique %>% nrow

d_unique %>% 
  ggplot(aes(x = CA)) +
  # TODO: binwidth or bins? Also, bins_ca * 2 or just bins_ca?
  geom_histogram(bins=bins_ca) +
  facet_wrap(Cond ~ .)
```

# <span style = "color:firebrick">Exercise 4:</span> Hypothesis testing [22 points]

## Ex 4.a Testing the "Learning Speed" hypothesis [6 points]

Test the "Learning Speed" hypothesis using a Bayesian linear regression model, following the assumption made in the paper that the measures in `Trials` are normally distributed.
Make sure to also record samples from the prior (even if we don't need them), and set the relevant prior for the slope term to a Student's $t$ distribution with 1 degree of freedom, mean 0 and standard deviation 100.
Test the hypothesis with `brms::hypothesis` and interpret the outcome with explicit mentioning of the quantities you base your interpretation on.

```{r, results='hide'}

d_unique <- d_unique %>%
  mutate(
    Cond = factor(Cond)
  )

fit_ls <- brm(
  formula = Trials ~ Cond,
  data = d_unique,
  prior = prior(student_t(1, 0, 100),  class = 'b'),
  sample_prior = 'yes',
  # TODO: change back to 20k
  iter = 1000
)
```

```{r}
(summary(fit_ls))
# mu_NV > mu_VB is our hypothesis
brms::hypothesis(fit_ls, "CondNV > 0")
```
** Analysis **
With an evidence ratio of around 2.5, we have evidence that is hardly worth ink or breath in favor of our hypothesis that learning speed is higher for the easily verbalized condition than for the non-easily verbalized condition (stated in 1.a). Also, the credible interval's lower bound is below the critical value of 0, as is the upper bound above 0. As such, the model and data provide no substantial evidence in favor of or against this hypothesis, so we withhold judgement. 

## Ex 4.b Testing the "Accuracy" hypothesis [8 points]

B&M test the "Accuracy" hypothesis based on the assumption that the measures in `Accuracy` are normally distributed. 
If you can, explain in one concise sentence why this normality assumption might be criticized in the case at hand.
If you cannot think of a different analysis you like better, follow this strategy to test the "Accuracy" hypothesis with a Bayesian regression model following the same general approach as in the previous exercise.
If you can think of an analysis you might prefer, do that (for 2 extra points).
Use the same priors as in the previous exercise.

** Solution **

We think that a normal distribution is inappropriate here, since the data is 
plausibly generated by a Bernoulli distribution.

```{r, results='hide'}
d_acc <- d_tidy %>%
  filter(str_detect(trial_label, "[AB]")) %>%
  group_by(Subj) %>% 
  mutate(correct = response == expected_response) %>%
  mutate(Cond = factor(Cond))
  

fit_acc <- brm(
  formula = correct ~ Cond,
  data = d_acc,
  family = bernoulli(link = "logit"),
  prior = prior(student_t(1, 0, 100), class='b'),
  sample_prior = 'yes',
  # TODO: change back to 20k
  iter = 1000
)
```

```{r}
summary(fit_acc)
# mu_NV < mu_VB is our hypothesis
brms::hypothesis(fit_acc, "CondNV < 0")
```
With an evidence ratio of Inf and a post. probability 1, we have decisive evidence in favor of our hypothesis that the accuracy is higher for the easily verbalized condition than for the not-easily verbalized condition. As such, we accept our hypothesis.
 
## Ex 4.c Testing the "Strategy" hypothesis [8 points]

Test the "Strategy" hypothesis in the way that you think is best appropriate (but using a method that we have covered in class).

** Solution **
Like in 4.b, we think that a normal distribution is inappropriate here, since the data is 
plausibly generated by a Bernoulli distribution.

```{r, results='hide'}
d_ca <- d_tidy %>%
  filter(!str_detect(trial_label, "[AB]")) %>%
  group_by(Subj) %>% 
  mutate(is_ca = response == expected_response) %>%
  mutate(Cond = factor(Cond))
  

fit_ca <- brm(
  formula = is_ca ~ Cond,
  data = d_ca,
  family = bernoulli(link = "logit"),
  prior = prior(student_t(1, 0, 100), class='b'),
  sample_prior = 'yes',
  # TODO: change back to 20k
  iter = 1000
)
```
```{r}
summary(fit_ca)
# mu_NV < mu_VB is our hypothesis
brms::hypothesis(fit_ca, "CondNV < 0")
# TODO: answer
```
With an evidence ratio of Inf and a post. probability 1, we have decisive evidence in favor of our hypothesis that the proportion of categorization choices for each subject, which are indicative of a ‘critical attribute’ categorization strategy, than in the “not-easily verbalizable” condition. As such, we accept our hypothesis.

# <span style = "color:firebrick">Exercise 5 (challenge question):</span> Comparison to ANOVA analysis [5 points]

B&M's ANOVA-based analysis for the "Strategy" Hypothesis actually also tested a fourth hypothesis, for which a significant test result was obtained.
Describe in words what that finding was.
Use conjugacy (starting from flat priors) to quickly sample from a Bayesian posterior to test this additional hypothesis using Kruschke's ternary logic for hypothesis testing.
(**Hint:** Did you really think you could take this final w/o addressing an issue translatable into a question about whether some stupid coin is fair or biased on some way or other?)

**Solution:**
The hypothesis tested for in the analysis of strategy model fit is whether model type (CA vs FR) has an influence on the model fit (scores), so whether there is a general preference for one strategy.
The reported main effect of category amounts to saying that participants seem to have favored choices indicative of a "characterizing attribute" (CA) strategy. 
This can be modeled as a coin flip: One either adopts method AC or method FR and there might be a total bias towards one strategy. With conjungate priors, we can assume a fair coin, add the data and then estimate what the bias parameter would need to be in order to generate this data. For that, we could
1. look at trials: how many of all decisions adopted a CA strategy? -> this would look at a general bias towards CA, not taking into consideration that bias would be distributed differently across subjects
2. look at subjects: how many of all subjects adopted a CA strategy? -> for this, we would need to somehow transform the score into a binary decision. This would alter the data, we would lose information and we would no longer look at the influence of model on the scores, but on a different variable -> probably not useful!
Afterwards, Kruschke can then help decide if this bias is believable or not. 
```{r}
```

$$
\begin{align*}
  P(\theta) & = Beta(1,1) \\
  P(\theta_c \mid N, k) & = \text{Beta}(\theta_c, 1+k, 1 + N - k) \\
\end{align*}
$$