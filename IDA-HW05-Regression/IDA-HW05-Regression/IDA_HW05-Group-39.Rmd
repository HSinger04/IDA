---
title: "Homework Sheet 5 -- Regression modeling"
date: 'Due: Wednesday, January 20 by 11:59 CET'
author: "39"
output: 
  html_document:
  toc: true
toc_depth: 2
highlight: tango
---
  
  ```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)

```

```{r libraries, include=FALSE, message=FALSE, warning=FALSE}

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for this course
library(aida)

# package for Bayesian regression
library(brms)

# parallel execution of Stan code
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values = project_colors)
} 

# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      cache = TRUE, fig.align = 'center')
```

# Instructions

**Important!** The `aida` package has been updated. Please run the following code to make sure your local version has these updates.

```{r, eval = F}
remotes::install_github("michael-franke/aida-package")
```

* Create an Rmd-file with your group number (equivalent to StudIP group) in the 'author' heading and answer the following questions.
* When all answers are ready, 'Knit' the document to produce a HTML file.
* Create a ZIP archive called "IDA_HW05-Group-XYZ.zip" (where 'XYZ' is *your* group number) containing:
  * an R Markdown file "IDA_HW05-Group-XYZ.Rmd"
* a knitted HTML document "IDA_HW05-Group-XYZ.html"
* any other files necessary to compile your Rmarkdown to HTML (data, pictures etc.)
* Upload the ZIP archive on Stud.IP in your group folder before the deadline (see above). You may upload as many times as you like before the deadline, only your final submission will count.
* If you run `brms::brm` in an R code chunk, make sure to include `results='hide'` as an option to that code chunk to suppress the output of Stan.

# <span style = "color:firebrick">Exercise 1:</span> Difference coding in Mental Chronometry [20 points]

The purpose of this exercise is to better understand coding schemes for categorical factors.
We also would like to get more comfortable with testing hypotheses about regression coefficients in a Bayesian setting.
Towards this end, we will construct a design matrix by hand to implement **simple difference coding**.
We use the non-informative Bayesian regression model to efficiently sample from the posterior and test hypotheses about the Mental Chronometry data.
In particular we are interested in the hypotheses spelled out in Appendix D.1.1.2 of the web-book, namely:
  
  - mean reaction times are lower for data from the 'reaction' block than for the 'goNoGo' block
- mean reaction times are lower for data from the 'goNoGo' block than for the 'discrimination' block

## Ex 1.a Prepare data [1 point]

Create a variable `data_MC_excerpt` that contains all and only the relevant columns from the MC data set.
The dependent variable should be the first column.

**Solution:**
  
```{r}
data_MC_excerpt <- aida::data_MC_cleaned %>% 
  select(RT, block)
data_MC_excerpt
```

## Ex 1.b Get the empirical differences between means [2 points]


Compute the following differences:
  
  1. the mean RT for condition `goNoGo` minus the mean RT for condition `reaction`
2. the mean RT for condition `discrimination` minus the mean RT for condition `goNoGo`

and store the result in the variables provided below (keep the brackets so that the values assigned show in the Rmarkdown output):
  
  **Solution:**
  
```{r}
data_MC_summary <- data_MC_excerpt %>%
  group_by(block) %>%
  summarise(mean = mean(RT))
(data_MC_summary)
(`goNoGo-reaction`       <-  data_MC_summary[2,2]-data_MC_summary[3,2])
(`discrimination-goNoGo` <-  data_MC_summary[1,2]-data_MC_summary[2,2])
```

## Ex 1.c Build predictor matrix [5 points]

Add three columns to the data tibble `data_MC_excerpt`, like shown below.
These columns should contain the numerical values to obtain so-called **simple difference coding** (as discussed in the initial video of Chapter 14, after ca. 10 minutes).

**Solution:**
  
```{r}
data_MC_excerpt <- data_MC_excerpt %>%
  group_by(block) %>%
  mutate(
    grand_mean  = 1, 
    `goNoGo-reaction` = ifelse(block == "discrimination", -2/3, 1/3),
    `discrimination-goNoGo` = ifelse(block == "reaction", 2/3, -1/3)
  )
```


## Ex 1.d Run a Bayesian regression model [3 points]

Use the function `aida::get_samples_regression_noninformative` to obtain 10,000 samples from the posterior distribution for a standard linear regression model.
(Hint: Use `as.matrix` to extract the predictor matrix from the tibble you created in the previous step.)

**Solution:**
  
```{r}
# leave out block col for matrix conversion since matrix values will be of
# type string instead of dbl otherwise
MC_matrix <- data_MC_excerpt[-2] 
MC_matrix <- MC_matrix %>% as.matrix
input_MC <- MC_matrix[,2:4]
output_MC <- MC_matrix[,1]
samples <- aida::get_samples_regression_noninformative(input_MC, output_MC, n_samples=10000)
```

## Ex 1.e Summary statistics [2 point]


Use the function `aida::summarize_sample_vector` to show Bayesian summary statistics for the two most relevant parameters (given the hypotheses we are interested in for the Mental Chronometry experiment).

**Solution:**
  
```{r}
(samples %>% 
    pull("goNoGo-reaction") %>% 
    aida::summarize_sample_vector(., name="goNoGo-reaction"))

(samples %>% 
    pull("discrimination-goNoGo") %>%
    aida::summarize_sample_vector(., name="discrimination-goNoGo"))
```

## Ex 1.f Interpret your results [7 points]

Interpret these results using a Kruschke-style logic applied to (non-ROPEd) interval-based hypotheses.
In doing so, state clearly what the coefficients encode, what the numerical results of the last step mean, and what you would conclude from all this regarding any potential evidence (or lack thereof) for or against the research hypotheses.

**Solution:**

TODO: possible needs to be corrected, see how the final coding looks like
We are looking at two hypotheses: That the 
* mean reaction time for ‘goNoGo’ minus the mean reaction time for ‘reaction’ is higher than 0
* mean reaction time for ‘discrimination’ minus the mean reaction time for ‘goNoGo’ is higher than 0

The coefficients shown in 1.e respectively show the mean and the 95% HDI of the approximation of the posterior distribution for 

* mean reaction time for ‘reaction’ minus mean reaction time for ‘goNoGo’
* mean reaction time for ‘goNoGo’ minus mean reaction time for ‘discrimination’

As we can clearly see that the additive inverse of the coefficients' HDIs is completely contained in $(0, \infty)$, according to a Kruschke-style logic, we would **accept** both our formulated hypotheses.   

# <span style = "color:firebrick">Exercise 2:</span> Analyzing the King of France [28 points]

The purpose of this exercise is to practice with logistic regression and the interpretation of interaction terms.
Thanks to those who took part, we have data from the King of France experiment from students of this class, so we can engage in "self-analysis".
If you want to understand what this experiment is about, read the relevant appendix chapter in the web-book.

We will be interested in the following research questions:

- **H1**: The latent probability of "TRUE" judgements is higher in C0 (with presupposition) than in C1 (where the presupposition is part of the at-issue / asserted content).
- **H2**: There is no difference in truth-value judgements between C0 (the positive sentence) and C6 (the negative sentence).
- **H3**: The disposition towards "TRUE" judgements is lower for C9 (where the presupposition is topical) than for C10 (where the presupposition is not topical and occurs under negation).

Load the data as follows (also applying some massaging, similar to what's done in the web-book):

```{r }
IDA_data_KoF <- read_csv('data_KoF-IDA-2020.csv') %>% 
  # discard practice trials
  filter(type != "practice") %>% 
  mutate(
    # add a 'condition' variable
    condition = case_when(
      type == "special" ~ "background check",
      type == "main" ~ str_c("Condition ", condition),
      TRUE ~ "filler"
    ) %>% 
      factor( 
        ordered = T,
        levels = c(str_c("Condition ", c(0, 1, 6, 9, 10)), "background check", "filler")
      )
  ) %>% 
  rename(correct_answer = correct)
IDA_data_KoF
```

## Ex 2.a Clean the data [3 points]

Just like in the web-book we are going to clean the data by performing the following two steps in sequence:

1. Remove all data from any participant who got more than 50% of the answer to filler material wrong.
2. Remove individual main trials if the corresponding "background check" question was answered wrongly.

Use code from the web-book at will, but be mindful that you may have to make (minor) changes (e.g., variable naming).

Use R code to report how many participants (in Step 1) and trials (in step 2) are excluded.

**Solution:**
```{r}
#1
subject_error_rate <- IDA_data_KoF %>% 
  # look at each participant in fillers only
  filter(condition == "filler") %>% 
  group_by(submission_id) %>% 
  # compute proportion of correct answers 
  summarise(
    proportion_correct = mean(correct_answer == response),
    outlier_subject = proportion_correct < 0.5
  ) %>% 
  arrange(proportion_correct) # keep only those who have TRUE boolean in proportion_correct

# keep only subjects who have a high enough subject_error_rate
d_cleaned <-
  full_join(IDA_data_KoF, subject_error_rate, by = "submission_id" ) %>% 
  filter(outlier_subject == FALSE)
# Compute how many subjects were excluded
nrow(IDA_data_KoF) - nrow(subject_error_rate) 


#2
before_cleanse <- nrow(d_cleaned)
d_cleaned <- d_cleaned %>% 
  filter(condition == "background check") %>% 
  mutate(
    background_correct = correct_answer == response
  ) %>% 
  select(submission_id, vignette, background_correct) %>% 
  right_join(d_cleaned, by = c("submission_id", "vignette")) %>% 
  # keep only the main trials which have a correct corresponding background check
  filter(type == "main" & background_correct == TRUE)
    
before_cleanse - nrow(d_cleaned)


```

## Ex 2.b Plot the data [2 points]

Plot the data just like in the the first part of the web-book's appendix D4.4 (proportion per condition). 
Use the web-book code as much as you like while making any necessary amendments.

**Solution:** TODO: They probably want figure D1 here :()
```{r}
IDA_data_KoF_plot <- IDA_data_KoF %>% 
  group_by(condition) %>% 
  summarise(num_condition = n()) %>% 
  mutate(proportion_condition = num_condition/nrow(IDA_data_KoF))
IDA_data_KoF_plot

d_cleaned %>% 
  # drop unused factor levels
  droplevels() %>% 
  # get means and 95% bootstrapped CIs for each condition
  group_by(condition) %>%
  nest() %>% 
  summarise(
    CIs = map(data, function(d) bootstrapped_CI(d$response == "TRUE"))
  ) %>% 
  unnest(CIs) %>% 
  # plot means and CIs
  ggplot(aes(x = condition, y = mean, fill = condition)) + 
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper, width = 0.2)) +
  ylim(0,1) +
  ylab("") + xlab("") + ggtitle("Proportion of 'TRUE' responses per condition") +
  theme(legend.position = "none") +
  scale_fill_manual(values = project_colors)

```

## Ex 2.c Run logistic regression model [5 points]

Run a logistic regression model on the cleaned data, predicting `response` in terms of categorical factor `condition`.
Make sure to also do the following:

- make the ordered factor `condition` a character vector before running the regression model
- change the non-informative default priors for all slope coefficients to a Student's t distribution with 1 degree of freedom, a mean of 0 and a standard deviation of 2 (see web-book Chapter 13.4)
- set the flap `sample_prior = 'yes'` to also sample from the prior
- take 20,000 samples (using parameter `iter` in the function call of `brm`)

Finally, also add a call of `summary(...)` to show the summary of the fitted model in the Rmarkdown output.

**Solution:**
```{r}
#make the ordered factor 'condition' a character vector
is.ordered(d_cleaned$condition)
d_cleaned <- d_cleaned %>%
  mutate(condition = as.character(condition))
d_cleaned
is.ordered(d_cleaned$condition)
#TODO: I thought they wanted to make sure condition is no longer ordered, but it was not in the first place. do you know what they want here?

kingOfFrance_fit = brm(
  formula = response ~ condition,
  family = bernoulli(link = "logit"),  # use Bernoulli as likelihood function because condition is a binary variable 
  data = d_cleaned,
  prior = prior(student_t(1, 0, 2), class = 'b'),
  sample_prior = 'yes',
  iter = 20000
)

summary(kingOfFrance_fit)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```


## Ex 2.d Test hypotheses [12 points]

Use the `brms::hypotheses` function to test the three hypotheses stated above.
For each case interpret the results and formulate a conclusion such as "does/does not provide evidence" or "accept/reject hypothesis" according to the results.
TODO: interpretation

### Hypothesis 1: C0 > C1

**Solution:**
```{r}
hypothesis(kingOfFrance_fit, "Intercept > conditionCondition1")
# TODO: intercept codes for condition0 right?
```


### Hypothesis 2: C0 = C6

**Solution:**
```{r}
hypothesis(kingOfFrance_fit, "Intercept = conditionCondition6")
```


### Hypothesis 3: C9 < C10

**Solution:**
```{r}
hypothesis(kingOfFrance_fit, "conditionCondition9 < conditionCondition10")
```


## Ex 2.e Interaction with `gender` [6 points]

Let's get nasty!
Let's do what no good scientist should ever do. (We do it for practice and for rubbing it in that this is NOT what you should EVER do.)

Suppose we did not like the outcome of the test for H1 above.
So we are going to run a new analysis (for which there is no *prima facie* rationale) just to see if we cannot possibly squeeze out something that looks like an argument for the conclusion that we so crave to be supported.
Worst of all, we stipulate -completely out of the blue- a potential gender effect on the understanding of linguistic presupposition (say what?).

Obviously, what this exercise is really after is getting practice with interpreting *interaction terms* for categorical regression.
So, look at the code below, try to understand what it does and then inspect the summary of the model fit shown here.
Then answer the following question: based on this analysis and results shown in the summary, is there any reason to believe that (i) there is a difference in the answer behavior between self-identified male and self-identified female participants (i.e., is there a main effect of `gender`?), and (ii) is there any specific way in which gender impacts on the disposition to select "TRUE" in either of the two conditions (i.e., is there an interaction?).
Identify the source of your conclusion by pointing out which numbers (or whatever) you draw on in the summary of the model fit.

```{r, results = 'hide'}
IDA_data_KoF_interaction <- IDA_data_KoF %>% 
  filter(condition %in% c("Condition 0", "Condition 1")) %>% 
  filter(!is.na(gender)) %>% 
  mutate(
    gender = factor(gender)
  ) %>% 
  droplevels() %>% 
  select(response, condition, gender)
  
# apply 'sum' contrasts
contrasts(IDA_data_KoF_interaction$condition) <- contr.sum(2)
contrasts(IDA_data_KoF_interaction$gender) <- contr.sum(2)

# add intelligible name to the new contrast coding
colnames(contrasts(IDA_data_KoF_interaction$condition)) <- ":Cond1"
colnames(contrasts(IDA_data_KoF_interaction$gender)) <- ":male"

fit_brms_KoF_interaction <- brm(
  # predict `response` in terms of `condition`
  formula = response ~ condition * gender,
  # specify which data to use
  data = IDA_data_KoF_interaction,
  # logistic regression
  family = bernoulli(link = "logit"),
  # weakly informative priors (slightly conservative)
  #   for `class = 'b'` (i.e., all slopes)
  prior = prior(student_t(1, 0, 2),  class = 'b'),
  # also collect samples from the prior (for point-valued testing)
  sample_prior = 'yes',
  # take more than the usual samples (for numerical stability of testing)
  iter = 20000
)
```


```{r}
summary(fit_brms_KoF_interaction)
```

**Solution:**
                                                                    